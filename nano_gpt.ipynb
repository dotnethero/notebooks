{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxYTVPdKojpFGrLRnuTO/p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMqa0qPpB9aV",
        "outputId": "750ea4e0-8fbe-4988-b717-ca1ac87ac516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax) (1.22.4)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax) (0.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.6.9)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from flax) (1.22.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.36)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.3.4)\n",
            "Requirement already satisfied: jax>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from flax) (4.5.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.2->flax) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.2->flax) (0.1.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.2->flax) (3.3.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.2.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.7)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.4.7+cuda11.cudnn86)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.5.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.5.6)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.2.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (5.12.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install jax\n",
        "!pip install flax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJbPb4dGlMO",
        "outputId": "59f38c1f-4336-4647-98e9-500cf6a116b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-09 10:16:37--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-09 10:16:37 (26.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(vocab_size)\n",
        "print(''.join(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSEGnridG5ZX",
        "outputId": "2629d52a-c2bf-479c-8df0-d1bc16f5f91e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos = { i: s for i, s in enumerate(vocab) }\n",
        "stoi = { s: i for i, s in enumerate(vocab) }\n",
        "\n",
        "encode = lambda txt: [stoi[c] for c in txt ]\n",
        "decode = lambda num: ''.join([itos[n] for n in num ])\n",
        "\n",
        "encoded = encode(\"The test string\")\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(encoded)\n",
        "print(decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoszAc6nHkxW",
        "outputId": "b51376b3-a364-4191-c017-5aa3a1f2523a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 46, 43, 1, 58, 43, 57, 58, 1, 57, 58, 56, 47, 52, 45]\n",
            "The test string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import Callable\n",
        "from jax import numpy as jnp\n",
        "from flax import linen as nn\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "YEml_yhcCF2m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = jnp.array(encode(text))\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loUgrT_wJWPM",
        "outputId": "0c9d6e49-9822-49b5-f941-884bf1618db7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_val = int(0.9 * len(data))\n",
        "train_data = data[:n_val]\n",
        "val_data = data[n_val:]\n",
        "\n",
        "train_data.shape, val_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPB6G1z8MB1N",
        "outputId": "f6f289e1-b3dd-43d0-9ad4-bfa536a625ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1003854,), (111540,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "batch_size = 32\n",
        "n_embed = 64\n",
        "\n",
        "def get_batch(batch_key, *, split='train'):\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = jax.random.randint(batch_key, (batch_size,), 0, len(d)-block_size)\n",
        "    x = jnp.stack([d[i:i+block_size] for i in ix])\n",
        "    y = jnp.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "key = jax.random.PRNGKey(1337)\n",
        "\n",
        "X, Y = get_batch(key)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDWSOGMuN3v0",
        "outputId": "962efb03-b9b4-485e-8e4c-f117f19d8903"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 128)\n",
            "(32, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    head_size: int = 32\n",
        "\n",
        "    wq_init: Callable = nn.initializers.lecun_normal()\n",
        "    wk_init: Callable = nn.initializers.lecun_normal()\n",
        "    wv_init: Callable = nn.initializers.lecun_normal()\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        \n",
        "        B, T, C = inputs.shape\n",
        "\n",
        "        # mask\n",
        "        ones = jnp.ones(shape=(T, T))\n",
        "        tril = jnp.tril(ones)\n",
        "        tril = jnp.stack([tril] * B)\n",
        "        \n",
        "        WQ = self.param(\"WQ\", self.wq_init, (C, self.head_size))\n",
        "        WK = self.param(\"WK\", self.wk_init, (C, self.head_size))\n",
        "        WV = self.param(\"WV\", self.wv_init, (C, self.head_size))\n",
        "        \n",
        "        Q = inputs @ WQ\n",
        "        K = inputs @ WK\n",
        "        V = inputs @ WV\n",
        "\n",
        "        KT = K.transpose([0, 2, 1]) # B, H, T\n",
        "        QK = Q @ KT / jnp.sqrt(self.head_size)\n",
        "        QK = jax.lax.select(tril == 0, jax.lax.broadcast(jnp.NINF, QK.shape), QK)\n",
        "\n",
        "        attention = nn.softmax(QK, axis=-1) @ V\n",
        "\n",
        "        return attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    embedding_size: int = 32\n",
        "    head_number: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        size = self.embedding_size // self.head_number\n",
        "        self.heads = [AttentionHead(head_size=size) for i in range(self.head_number)]\n",
        "        self.proj = nn.Dense(self.embedding_size)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        x = [h(x) for h in self.heads]\n",
        "        x = jnp.concatenate(x, axis=-1)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    embedding_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        x = nn.Dense(4 * self.embedding_size)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.embedding_size)(x)\n",
        "        return x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    embedding_size: int = 32\n",
        "    head_number: int = 4\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = x + MultiHeadAttention(embedding_size=self.embedding_size, head_number=self.head_number)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = x + FeedForward(embedding_size=self.embedding_size)(x)\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \n",
        "    vocab_size: int\n",
        "    block_size: int\n",
        "    embedding_size: int = 32\n",
        "    head_number: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        self.token_embedding = nn.Embed(self.vocab_size, self.embedding_size)\n",
        "        self.position_embedding = nn.Embed(self.block_size, self.embedding_size)\n",
        "        self.blocks = nn.Sequential([\n",
        "            AttentionBlock(embedding_size=self.embedding_size, head_number=self.head_number),\n",
        "            AttentionBlock(embedding_size=self.embedding_size, head_number=self.head_number),\n",
        "            nn.LayerNorm()\n",
        "        ])\n",
        "        self.lm_head = nn.Dense(self.vocab_size)\n",
        "    \n",
        "    def __call__(self, idx):\n",
        "        token_embeddings = self.token_embedding(idx) # B, T, C\n",
        "\n",
        "        positions = jnp.arange(0, self.block_size)\n",
        "        positions_embeddings = self.position_embedding(positions) # T, C\n",
        "\n",
        "        embeddings = token_embeddings + positions_embeddings\n",
        "        attention = self.blocks(embeddings)\n",
        "        logits = self.lm_head(attention)\n",
        "\n",
        "        return logits\n",
        "\n",
        "@jax.jit\n",
        "def bigram_loss(y_hat, y):\n",
        "    losses = optax.softmax_cross_entropy_with_integer_labels(y_hat, y)\n",
        "    return jnp.mean(losses)\n",
        "\n",
        "def bigram_generate(apply_fn: Callable, key, idx, max_tokens=1):\n",
        "    for i in range(max_tokens):\n",
        "        key_i = jax.random.fold_in(key, i)\n",
        "        logits = apply_fn(params, idx[:, -block_size:])\n",
        "        logits = logits[:, -1, :] # B, C\n",
        "        probs = nn.softmax(logits, axis=-1) # B, C\n",
        "        idx_next = jax.random.categorical(key_i, logits, axis=-1) # B\n",
        "        idx_next = jnp.expand_dims(idx_next, -1)\n",
        "        idx = jnp.concatenate([idx, idx_next], axis=1) # B, T+1\n",
        "    return idx\n",
        "\n",
        "@partial(jax.jit, static_argnums=[0])\n",
        "def train_step(apply_fn: Callable, params, opt_state, x_batch, y_batch):\n",
        "\n",
        "    @jax.jit\n",
        "    def loss_fn(params, x_batch, y_batch):\n",
        "        y_hat = apply_fn(params, x_batch)\n",
        "        return bigram_loss(y_hat, y_batch)\n",
        "\n",
        "    loss, grad = jax.value_and_grad(loss_fn)(params, x_batch, y_batch)\n",
        "    updates, opt_state = opt.update(grad, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n"
      ],
      "metadata": {
        "id": "odzKm4ItQSd5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(1337)\n",
        "\n",
        "model = BigramLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_size=n_embed)\n",
        "params = model.init(key, X)\n",
        "\n",
        "out = model.apply(params, X)\n",
        "print(out.shape)\n",
        "\n",
        "loss = bigram_loss(out, Y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joSWKRn_Dwqv",
        "outputId": "c939828e-589a-4021-b41d-922d18d1180e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 128, 65)\n",
            "4.5697713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(1337)\n",
        "key_init, key_shuffle = jax.random.split(key)\n",
        "\n",
        "model = BigramLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_size=n_embed)\n",
        "params = model.init(key, X)\n",
        "\n",
        "opt = optax.adam(1e-3)\n",
        "opt_state = opt.init(params)"
      ],
      "metadata": {
        "id": "Pfq-LkmKY_o5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(10000)):\n",
        "    key_batch = jax.random.fold_in(key=key_shuffle, data=epoch)\n",
        "    X, Y = get_batch(key_batch)\n",
        "    params, opt_state, loss = train_step(model.apply, params, opt_state, X, Y)\n",
        "\n",
        "print(f\"\\n\\nLoss: {loss}\") # Loss: 2.127, 1.985"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86pVbzglSjr",
        "outputId": "3fe640ca-fea1-463d-8c53-5a0fce96832d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [1:06:58<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Loss: 1.4453058242797852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z = jnp.zeros(shape=(1, block_size), dtype=jnp.int32)\n",
        "out = bigram_generate(model.apply, key_shuffle, Z, max_tokens=500)\n",
        "out = decode(out[0][block_size:].tolist())\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6fHtIALituo",
        "outputId": "06a98c22-4599-4c93-bbb0-336492e608f0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KINGNurgivining:\n",
            "Ould call it.\n",
            "\n",
            "ROMEO:\n",
            "You do gooy what ever, he, not unjure woul of thing\n",
            "As Gaunts! Let's my dage deeming show't!\n",
            "Give to this nine; mars; old so you:\n",
            "Upon are are the helm, she bot to the guess,\n",
            "By look my present larrow to wait, is this doo fight\n",
            "His art looking to ancing about;\n",
            "If your go go my goads as wilfords: the would discenctor?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Respass it.\n",
            "\n",
            "LEONTES:\n",
            "It city.\n",
            "Dive to them as the quilent whence.\n",
            "\n",
            "COMEPE:\n",
            "Would may.\n",
            "\n",
            "HERMIONE:\n",
            "Umilly tenchme, thou disson\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6mQWBChCVP6"
      },
      "execution_count": 102,
      "outputs": []
    }
  ]
}